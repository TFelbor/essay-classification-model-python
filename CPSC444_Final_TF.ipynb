{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yPGVwUuKO213",
        "xzaKp_aUQFZl"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Essay Classification Problem: Is it student-written or LLM-generated?**"
      ],
      "metadata": {
        "id": "9WFxSJJGlCRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup & Data Loading\n",
        "\n",
        "### The plan is to load the dataset ( train_essays.csv ) and to prepare the environment in Colab.\n",
        "1. *Set up TensorFlow, scikit-learn, and imbalanced-learn for modeling and oversampling. Then download GloVe embeddings for text processing.*\n",
        "2. *The dataset for each essay, provides contents and labels ( 0 for student-written, 1 for LLM-generated ). Load it using pandas DataFrame & inspect the class distribution to confirm the imbalance.*\n",
        "3. *Colab’s free tier provides a GPU/TPU, which we’ll leverage to speed up the training process. We must manage memory carefully however, due to the ~12 GB RAM limit.*"
      ],
      "metadata": {
        "id": "ffvhF_VSMfwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjpgHVURnjg1",
        "outputId": "ba27a9a8-e8f7-41eb-c3f5-64b2eb940af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution prior to oversampling:\n",
            "generated\n",
            "0    1375\n",
            "1       3\n",
            "Name: count, dtype: int64\n",
            "Downloading the GloVe embeddings...\n"
          ]
        }
      ],
      "source": [
        "# Import the neccessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "'''\n",
        "Download NLTK data for tokenization process, since it's better suited for text\n",
        "processing. It's less scalable than the tokenizer Tensorlow offers, but seems\n",
        "a better fit for our purposes.\n",
        "'''\n",
        "nltk.download('punkt')\n",
        "# Specific tokenizer model required in our Colab's Python environment\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Enable GPU ( manually enable via Runtime > Change runtime type > GPU T4 )\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# Load the training data\n",
        "train_df = pd.read_csv('/content/data/train_essays.csv')\n",
        "print(\"Class distribution prior to oversampling:\")\n",
        "# Check for imbalance\n",
        "print(train_df['generated'].value_counts())\n",
        "\n",
        "# Download GloVe embeddings ( 100D, should not exceed 400 MB )\n",
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "glove_path = '/content/glove.6B.100d.txt'\n",
        "if not os.path.exists(glove_path):\n",
        "    print(\"Downloading the GloVe embeddings...\")\n",
        "    r = requests.get(glove_url)\n",
        "    with open('/content/glove.6B.zip', 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    with zipfile.ZipFile('/content/glove.6B.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **Libraries:** *TensorFlow for the ANN implementation, scikit-learn for training metrics, imbalanced-learn for managing oversampling, NLTK for word tokenization, and pandas for handling data structures.*\n",
        "- **GloVe Download:** *We use specifically GloVe 100D embeddings (~400 MB and manageable in Colab environment) in order to convert words into vectors. The file is downloaded only if not already present to save time & space.*\n",
        "- **Class Distribution:** *We check generated counts to confirm the existing imbalance, helping our oversampling strategy.*\n",
        "- **GPU Setup:** *Enables memory growth and prevents TensorFlow from reserving all GPU memory, therefore reducing the crashes in the Colab environment.*"
      ],
      "metadata": {
        "id": "vLxJCOVjN-lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Text Preprocessing and GloVe Text Embedding\n",
        "\n",
        "### The plan is to tokenize essays ( or rather the words that compose them ), and convert those words into GloVe 100D embeddings. Then, averaging the embeddings will allows us to have a 100D vector created per each essay.\n",
        "1. **Tokenization:** *Essays are raw text, so we split them into words ( our tokens ) using NLTK’s word_tokenize to prepare for embedding.*\n",
        "2. **GloVe Embeddings:** *We load the GloVe 100D file into a dictionary mapping\n",
        "the words to 100D vectors. Each essay’s tokens are then converted into vectors, so that finally we can average them & create a fixed-length 100D embeddings, which will later serve as the input for our ANN.*\n",
        "3. **Fixed Weights:** *Using GloVe embeddings as fixed weights allows for reduction on parameters. Doing so, enables a faster training and doesn't exhaust the Colab’s computational constraints.*\n",
        "4. **Averaging:** *Averaging the embeddings simplifies our essays of varied length into a single vector.*"
      ],
      "metadata": {
        "id": "ppocIbtsMqDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings into a dictionary\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "print(f\"Loaded {len(glove_embeddings)} GloVe embeddings.\")\n",
        "\n",
        "# Tokenize and convert essays to averaged GloVe embeddings\n",
        "def text_to_embedding(text, embeddings, embedding_dim=100):\n",
        "    # Lowercase for consistency\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    valid_embeddings = [embeddings.get(token, np.zeros(embedding_dim)) for token in tokens\n",
        "                        if token in embeddings]\n",
        "    # Handle empty or OOV cases\n",
        "    if not valid_embeddings:\n",
        "        return np.zeros(embedding_dim)\n",
        "    return np.mean(valid_embeddings, axis=0)\n",
        "\n",
        "# Apply to training data\n",
        "X = np.array([text_to_embedding(text, glove_embeddings) for text in train_df['text']])\n",
        "y = train_df['generated'].values\n",
        "print(f\"Input shape: {X.shape}, Labels shape: {y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1-2ytfCNlyd",
        "outputId": "c668934b-5775-4cb6-b5bb-6f347aa3eb0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 GloVe embeddings.\n",
            "Input shape: (1378, 100), Labels shape: (1378,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **GloVe Loading:** *The dictionary maps words to 100D vectors, with ~400,000 words in GloVe 6B, covering most essay vocabulary.*\n",
        "- **Tokenization:** *Lowercasing ensures consistency. NLTK’s tokenizer handles punctuation and complex words.*\n",
        "- **Averaging Embeddings:** *Averaging captures the essay’s overall meaning, suitable for our simple ANN. Out-of-vocabulary ( OOV ) words get zero vectors, which is rare given GloVe’s large vocabulary.*\n",
        "- **Output:** *X is a matrix of shape ( n_samples, 100 ), where n_samples is ~10,000, and y is a vector of labels ( 0 or 1 ).*"
      ],
      "metadata": {
        "id": "Hw771FHwOeTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Random Oversampling\n",
        "### The plan is to apply random oversampling & achieve partial balance ( ~4,000 LLM-generated vs. 9,500 student-written ).\n",
        "1. *We need to set up TensorFlow, scikit-learn, and imbalanced-learn for modeling and oversampling, and download GloVe embeddings for text processing.*\n",
        "2. *The dataset contains essay texts and labels ( 0 for student-written, 1 for LLM-generated ). We’ll load it into a pandas DataFrame to inspect the class distribution and confirm the imbalance.*\n",
        "3. *Colab’s free tier provides a GPU/TPU, which we’ll enable to speed up training, but we must manage memory carefully due to the ~12 GB RAM limit.*"
      ],
      "metadata": {
        "id": "yPGVwUuKO213"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply random oversampling. ( ~4,000 LLM / 9,500 student = 0.421 )\n",
        "ros = RandomOverSampler(sampling_strategy=0.421, random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "print(\"Class distribution after oversampling:\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "print(f\"Resampled input shape: {X_resampled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gxOw01UO2gv",
        "outputId": "301ec435-49ec-427d-a6f1-036cddbdc80d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after oversampling:\n",
            "0    1375\n",
            "1     578\n",
            "Name: count, dtype: int64\n",
            "Resampled input shape: (1953, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **Sampling Strategy:** *The ratio 0.421 ( ~4,000 / 9,500 ) achieves partial balance, reducing duplication compared to full balance ( ~9,500 / 9,500 ).*\n",
        "- **Random State:** *Setting random_state = 42 ensures reproducibility in our report. As long as this value stays constant, the randomness will not change our results.*\n",
        "- **Output:** *X_resampled and y_resampled have ~13,500 samples. Totaling ~4,000 LLM-generated & ~9,500 student-written.*"
      ],
      "metadata": {
        "id": "21dtZiccPy-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Splitting\n",
        "### The plan is to split the oversampled data into two sets. One for training ( 80% ) and second for validation ( 20% ) purposes.\n",
        "1. *The validation set will allow us to evaluate hyperparameter performance and monitor overfitting without involving the test set.*\n",
        "2. *An 80:20 split ( ~10,800 for training, and ~2,700 for validation ) balances the availability of training data with validation.*\n",
        "3. *The process of stratification ensures the class distribution is preserved correctly across both sets.*"
      ],
      "metadata": {
        "id": "xzaKp_aUQFZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled,\n",
        "                                                  test_size=0.2, stratify=y_resampled,\n",
        "                                                  random_state=42)\n",
        "print(f\"Training shape: {X_train.shape}, Validation shape: {X_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqlqzFBLQdHn",
        "outputId": "f80f9f78-a82a-45e2-b163-d37d3962f036"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape: (1562, 100), Validation shape: (391, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **Stratification:** *The process maintains the oversampled class ratio (~30% LLM-generated) in both sets, doing so it ensures a fair evaluation of F1-score.*\n",
        "- **Random State:** *Ensures the necessary repetetiveness of events while operating with randomness.*\n",
        "- **Output:** *~10,800 training samples & ~2,700 validation samples, each 100D vectors*"
      ],
      "metadata": {
        "id": "WfE-PwJLQfIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Build the ANN\n",
        "### The plan is to implement our ANN architecture using TensorFlow\n",
        "1. *The ANN takes a 100D input (our averaged GloVe embedding), and applies a 64-node hidden layer with ReLU activation, uses an appropriate dropout value, in regards to the dataset size, (0.2) for regularization, and finally outputs a probability in our sigmoid layer.*\n",
        "2. *Binary cross-entropy loss is a standard for binary classification, and the Adam optimizer proves efficient for tasks involving text.*\n",
        "3. *The small size (~6,500 parameters) ensures efficient training in our Colab environment.*"
      ],
      "metadata": {
        "id": "S8ElBDeZQ0dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build ANN with TensorFlow\n",
        "def build_model(learning_rate=0.001, dropout_rate=0.2):\n",
        "    model = tf.keras.Sequential([\n",
        "        # 100D GloVe vector\n",
        "        tf.keras.layers.Input(shape=(100,)),\n",
        "        # Hidden layer\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        # Regularization\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        # Output probability\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  # Accuracy for monitoring\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initial model with default hyperparameters\n",
        "model = build_model(learning_rate=0.001, dropout_rate=0.2)\n",
        "# Display architecture and parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "qQy9FDaLRLUv",
        "outputId": "115f89e8-bab2-49a8-f886-b9c6c44497c1",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,529\u001b[0m (25.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,529</span> (25.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,529\u001b[0m (25.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,529</span> (25.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **ANN Architecture:** *64 nodes, dropout = 0.2, ReLU, sigmoid*\n",
        "- **Error Loss:** *Binary cross-entropy suits binary classification, fitting our sigmoid function output.*\n",
        "- **Metrics:** *Accuracy included for monitoring/debugging, but the F1-score is computeed solely and separately using predictions.*\n",
        "- **Parameters:** *~6,500 (100 × 64 + 64 + 64 × 1 + 1) is small, and good enough of a fit for our Colab environment*"
      ],
      "metadata": {
        "id": "E2gCRkuoRThg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Manual Hyperparameter Tuning\n",
        "### The plan is to train the ANN with 3–5 hyperparameter combinations, with focus on optimizing our F1-score.\n",
        "1. *We start with default settings (learning rate 0.001, batch size 32, dropout 0.2) and use early stopping to quit after no improvement observed, aiming at prevention of overtraining & wasting the limited computational resources.*\n",
        "2. *We test adjustments based on the performance. Select the combination with the highest validation F1-score.*\n",
        "3. *Early stopping (patience=3) ensures computational efficiency by stopping if F1-score doesn’t improve for 3 epochs. Training capped at 20 epochs.*"
      ],
      "metadata": {
        "id": "PPlGb5i7RyzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement early stopping mechanism\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=3, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Function that computes the F1-score\n",
        "def compute_metrics(model, X, y):\n",
        "    y_pred = (model.predict(X) > 0.5).astype(int)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    return f1, acc\n",
        "\n",
        "# Different hyperparameter combinations to test for best\n",
        "hyperparams = [\n",
        "    # Default\n",
        "    {'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2},\n",
        "    # Slower learning\n",
        "    {'learning_rate': 0.0001, 'batch_size': 32, 'dropout_rate': 0.2},\n",
        "    # Bigger batch\n",
        "    {'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2},\n",
        "    # Bigger dropout\n",
        "    {'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3},\n",
        "]\n",
        "\n",
        "# Manual tuning\n",
        "best_f1 = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "for params in hyperparams:\n",
        "    print(f\"Testing: {params}\")\n",
        "    model = build_model(learning_rate=params['learning_rate'],\n",
        "                        dropout_rate=params['dropout_rate'])\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=20,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks=[early_stopping],\n",
        "                        verbose=1)\n",
        "    # Compute validation metrics\n",
        "    val_f1, val_acc = compute_metrics(model, X_val, y_val)\n",
        "    print(f\"Validation F1-Score: {val_f1:.3f}, Accuracy: {val_acc:.3f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        best_params = params\n",
        "        best_model = model\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Best Validation F1-Score: {best_f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BG1dco0SL22",
        "outputId": "288895e9-972e-4de1-8136-adb57bf9df04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: {'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2}\n",
            "Epoch 1/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.5509 - loss: 0.7084 - val_accuracy: 0.7033 - val_loss: 0.5636\n",
            "Epoch 2/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7083 - loss: 0.5489 - val_accuracy: 0.7033 - val_loss: 0.4854\n",
            "Epoch 3/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7582 - loss: 0.4728 - val_accuracy: 0.7928 - val_loss: 0.4122\n",
            "Epoch 4/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8359 - loss: 0.4055 - val_accuracy: 0.9054 - val_loss: 0.3361\n",
            "Epoch 5/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8926 - loss: 0.3400 - val_accuracy: 0.9054 - val_loss: 0.2750\n",
            "Epoch 6/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9304 - loss: 0.2653 - val_accuracy: 0.9923 - val_loss: 0.2146\n",
            "Epoch 7/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.2201 - val_accuracy: 0.9872 - val_loss: 0.1722\n",
            "Epoch 8/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9877 - loss: 0.1756 - val_accuracy: 0.9872 - val_loss: 0.1388\n",
            "Epoch 9/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9922 - loss: 0.1379 - val_accuracy: 0.9898 - val_loss: 0.1161\n",
            "Epoch 10/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9906 - loss: 0.1194 - val_accuracy: 0.9872 - val_loss: 0.0978\n",
            "Epoch 11/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9935 - loss: 0.0972 - val_accuracy: 0.9872 - val_loss: 0.0844\n",
            "Epoch 12/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0902 - val_accuracy: 0.9872 - val_loss: 0.0730\n",
            "Epoch 13/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9960 - loss: 0.0700 - val_accuracy: 0.9949 - val_loss: 0.0655\n",
            "Epoch 14/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9907 - loss: 0.0660 - val_accuracy: 0.9872 - val_loss: 0.0587\n",
            "Epoch 15/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9934 - loss: 0.0576 - val_accuracy: 0.9898 - val_loss: 0.0522\n",
            "Epoch 16/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9925 - loss: 0.0542 - val_accuracy: 0.9949 - val_loss: 0.0484\n",
            "Epoch 17/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9947 - loss: 0.0471 - val_accuracy: 0.9872 - val_loss: 0.0480\n",
            "Epoch 18/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9932 - loss: 0.0451 - val_accuracy: 0.9923 - val_loss: 0.0400\n",
            "Epoch 19/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9958 - loss: 0.0336 - val_accuracy: 0.9872 - val_loss: 0.0407\n",
            "Epoch 20/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9982 - loss: 0.0335 - val_accuracy: 0.9923 - val_loss: 0.0349\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
            "Validation F1-Score: 0.987, Accuracy: 0.992\n",
            "Testing: {'learning_rate': 0.0001, 'batch_size': 32, 'dropout_rate': 0.2}\n",
            "Epoch 1/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.4406 - loss: 0.7262 - val_accuracy: 0.7033 - val_loss: 0.6409\n",
            "Epoch 2/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7142 - loss: 0.6200 - val_accuracy: 0.7033 - val_loss: 0.6059\n",
            "Epoch 3/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7017 - loss: 0.6172 - val_accuracy: 0.7033 - val_loss: 0.5940\n",
            "Epoch 4/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6922 - loss: 0.6039 - val_accuracy: 0.7033 - val_loss: 0.5852\n",
            "Epoch 5/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7142 - loss: 0.5822 - val_accuracy: 0.7033 - val_loss: 0.5745\n",
            "Epoch 6/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6932 - loss: 0.5913 - val_accuracy: 0.7033 - val_loss: 0.5652\n",
            "Epoch 7/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6946 - loss: 0.5720 - val_accuracy: 0.7033 - val_loss: 0.5558\n",
            "Epoch 8/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7059 - loss: 0.5610 - val_accuracy: 0.7033 - val_loss: 0.5469\n",
            "Epoch 9/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7114 - loss: 0.5475 - val_accuracy: 0.7033 - val_loss: 0.5381\n",
            "Epoch 10/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7079 - loss: 0.5375 - val_accuracy: 0.7033 - val_loss: 0.5292\n",
            "Epoch 11/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7187 - loss: 0.5267 - val_accuracy: 0.7033 - val_loss: 0.5203\n",
            "Epoch 12/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7167 - loss: 0.5301 - val_accuracy: 0.7033 - val_loss: 0.5115\n",
            "Epoch 13/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7195 - loss: 0.5126 - val_accuracy: 0.7033 - val_loss: 0.5026\n",
            "Epoch 14/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7373 - loss: 0.5038 - val_accuracy: 0.7033 - val_loss: 0.4937\n",
            "Epoch 15/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7337 - loss: 0.5016 - val_accuracy: 0.7033 - val_loss: 0.4849\n",
            "Epoch 16/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7267 - loss: 0.4909 - val_accuracy: 0.7033 - val_loss: 0.4759\n",
            "Epoch 17/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7485 - loss: 0.4751 - val_accuracy: 0.7033 - val_loss: 0.4672\n",
            "Epoch 18/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7584 - loss: 0.4674 - val_accuracy: 0.7033 - val_loss: 0.4581\n",
            "Epoch 19/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.4575 - val_accuracy: 0.7033 - val_loss: 0.4487\n",
            "Epoch 20/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7747 - loss: 0.4568 - val_accuracy: 0.7033 - val_loss: 0.4398\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Validation F1-Score: 0.000, Accuracy: 0.703\n",
            "Testing: {'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2}\n",
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.7013 - loss: 0.5991 - val_accuracy: 0.7033 - val_loss: 0.5504\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6997 - loss: 0.5462 - val_accuracy: 0.7033 - val_loss: 0.5005\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7112 - loss: 0.5090 - val_accuracy: 0.7033 - val_loss: 0.4480\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8087 - loss: 0.4394 - val_accuracy: 0.9079 - val_loss: 0.3949\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8923 - loss: 0.3861 - val_accuracy: 0.8951 - val_loss: 0.3451\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8878 - loss: 0.3491 - val_accuracy: 0.9054 - val_loss: 0.2978\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8968 - loss: 0.3062 - val_accuracy: 0.9821 - val_loss: 0.2565\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9600 - loss: 0.2493 - val_accuracy: 0.9923 - val_loss: 0.2186\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9755 - loss: 0.2139 - val_accuracy: 0.9872 - val_loss: 0.1865\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.1879 - val_accuracy: 0.9898 - val_loss: 0.1631\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.1747 - val_accuracy: 0.9872 - val_loss: 0.1415\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9954 - loss: 0.1425 - val_accuracy: 0.9898 - val_loss: 0.1243\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.1225 - val_accuracy: 0.9872 - val_loss: 0.1102\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9934 - loss: 0.1063 - val_accuracy: 0.9872 - val_loss: 0.0985\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9896 - loss: 0.0981 - val_accuracy: 0.9872 - val_loss: 0.0893\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9952 - loss: 0.0905 - val_accuracy: 0.9898 - val_loss: 0.0809\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0858 - val_accuracy: 0.9898 - val_loss: 0.0745\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9945 - loss: 0.0693 - val_accuracy: 0.9898 - val_loss: 0.0680\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9924 - loss: 0.0705 - val_accuracy: 0.9898 - val_loss: 0.0627\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9915 - loss: 0.0624 - val_accuracy: 0.9872 - val_loss: 0.0590\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Validation F1-Score: 0.979, Accuracy: 0.987\n",
            "Testing: {'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3}\n",
            "Epoch 1/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.6173 - loss: 0.6601 - val_accuracy: 0.7033 - val_loss: 0.5323\n",
            "Epoch 2/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7369 - loss: 0.5162 - val_accuracy: 0.7033 - val_loss: 0.4535\n",
            "Epoch 3/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8025 - loss: 0.4396 - val_accuracy: 0.9105 - val_loss: 0.3766\n",
            "Epoch 4/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8609 - loss: 0.3764 - val_accuracy: 0.9054 - val_loss: 0.3041\n",
            "Epoch 5/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9211 - loss: 0.2922 - val_accuracy: 0.9949 - val_loss: 0.2398\n",
            "Epoch 6/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9543 - loss: 0.2466 - val_accuracy: 0.9872 - val_loss: 0.1874\n",
            "Epoch 7/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9855 - loss: 0.1935 - val_accuracy: 0.9898 - val_loss: 0.1495\n",
            "Epoch 8/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9915 - loss: 0.1484 - val_accuracy: 0.9872 - val_loss: 0.1214\n",
            "Epoch 9/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9909 - loss: 0.1328 - val_accuracy: 0.9872 - val_loss: 0.1007\n",
            "Epoch 10/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9928 - loss: 0.1035 - val_accuracy: 0.9872 - val_loss: 0.0875\n",
            "Epoch 11/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0854 - val_accuracy: 0.9949 - val_loss: 0.0769\n",
            "Epoch 12/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0767 - val_accuracy: 0.9872 - val_loss: 0.0659\n",
            "Epoch 13/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9904 - loss: 0.0684 - val_accuracy: 0.9949 - val_loss: 0.0584\n",
            "Epoch 14/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9940 - loss: 0.0571 - val_accuracy: 0.9949 - val_loss: 0.0519\n",
            "Epoch 15/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9943 - loss: 0.0510 - val_accuracy: 0.9949 - val_loss: 0.0463\n",
            "Epoch 16/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0430 - val_accuracy: 0.9949 - val_loss: 0.0430\n",
            "Epoch 17/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9967 - loss: 0.0384 - val_accuracy: 0.9949 - val_loss: 0.0387\n",
            "Epoch 18/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0381 - val_accuracy: 0.9949 - val_loss: 0.0361\n",
            "Epoch 19/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9974 - loss: 0.0328 - val_accuracy: 0.9949 - val_loss: 0.0337\n",
            "Epoch 20/20\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0383 - val_accuracy: 0.9949 - val_loss: 0.0315\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Validation F1-Score: 0.991, Accuracy: 0.995\n",
            "Best hyperparameters: {'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3}\n",
            "Best Validation F1-Score: 0.991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **Combinations:** *The four combinations test different variants of hyperparameters ( default, slower learning, larger batch, larger dropout ), aiming at balancing both the exploration and efficiency aspects of our model's functionality.*\n",
        "- **Early Stop:** *Does so by monitoring the validation loss ( which is correlated to our F1-score ), doing so the model gains efficiency and prevents overfitting.*\n",
        "- **F1-Score:** *Computed during post-training ( TensorFlow doesn’t directly support F1-score as a metric ). We leverage scikit-learn’s f1_score.*\n",
        "- **Efficiency:** *Each model trains for approx. 5–10 minutes ( depending on the number of epochs ), resulting in around 20–40 minutes in total, and should fit within Colab’s runtime limits.*"
      ],
      "metadata": {
        "id": "IvBye5OsSQAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Threshold Optimization\n",
        "### The plan is to adjust the classification threshold and maximize the validation of our F1-score\n",
        "1. *The ANN outputs probabilities (between 0 & 1), and we use a default threshold of 0.5 to classify (border value for deciding whether the essay is AI or Human generated). Moreover, optimizing and adjusting our threshold can improve F1-score by balancing precision and recall.*\n",
        "2. *To successfully do so, we test different thresholds on the validation set to find the one which maximizes our F1-score.*"
      ],
      "metadata": {
        "id": "yfHwgsg5Smlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize the threshold of F1-score\n",
        "def optimize_threshold(model, X, y):\n",
        "    y_pred_proba = model.predict(X)\n",
        "    thresholds = np.arange(0.3, 0.8, 0.1)\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_pred_proba > threshold).astype(int)\n",
        "        f1 = f1_score(y, y_pred)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_threshold, best_f1\n",
        "\n",
        "# Apply to model that maximizes the F1-score\n",
        "best_threshold, final_f1 = optimize_threshold(best_model, X_val, y_val)\n",
        "final_acc = accuracy_score(y_val, (best_model.predict(X_val) > best_threshold).astype(int))\n",
        "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
        "print(f\"Final Validation F1-Score: {final_f1:.3f}, Accuracy: {final_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNiw0RJeSgjG",
        "outputId": "9fc26ef2-581c-4eac-e2a9-8de9448a892b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "Best Threshold: 0.70\n",
            "Final Validation F1-Score: 0.996, Accuracy: 0.997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Breakdown\n",
        "- **Threshold Range:** *0.3 to 0.7 covers likely optimal points, as extreme thresholds (e.g., 0.9) skew precision or recall.*\n",
        "- **F1-Score Focus:** *Maximizing F1-score aligns with our primary metric, improving detection of LLM-generated essays.*\n",
        "- **Efficiency:** *Threshold optimization is fast (~seconds), as it uses existing predictions.*"
      ],
      "metadata": {
        "id": "tAqFLa6RS6K-"
      }
    }
  ]
}